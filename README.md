<p align = "center" draggable=”false” ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>


<h1 align="center" id="heading">:wave: Welcome to LLM Engineering - Foundations to SLMs!</h1>

Recordings at - https://www.youtube.com/playlist?list=PLrSHiQgy4VjEquNgyyjatUUF22lqARHrI

Large Language Model Engineering (LLM Engineering) refers to the emerging best-practices and tools for pretraining, post-training, and optimizing LLMs prior to production deployment.

Pre- and post-training techniques include unsupervised pretraining, supervised fine-tuning, alignment, model merging, distillation, quantization. and others.

*LLM Engineering today is done with the GPT-style transformer architecture.

**Small Language Models (SLMs) of today can be as large as 70B parameters.

## Course Modules
This course teaches you the fundamentals of LLMs, and will quickly onramp you up to the practical LLM Engineering edge.  When you complete this course, you will understand how the latest Large and Small Language Models are built, and you'll be ready to build, ship, and share your very own.  <br/>
### Module 1: Transformer: Attention Is All You Need
🤖 The Transformer <br/>
🧐 Attention <br/>
🔠 Embeddings <br/>
### Module 2: Practical LLM Mechanics
🪙 Next-Token Prediction <br/>
🔡 Embedding Models <br/>
### Module 3: LLM Training, Fine-Tuning, and Alignment
🚇 Pretraining <br/>
🚉 Fine-Tuning <br/>
🛤️ Alignment <br/>
### Module 4: LLM Engineering Frontiers
🥪 Model Merging <br/>
⚗️ Distillation <br/>

and more from the LLM Edge! <br/>

## 🙏 Contributions

We believe in the power of collaboration. Contributions, ideas, and feedback are highly encouraged! Let's build the ultimate resource for LLMEs together. 🤝

Feel free to reach out with any questions or suggestions. Happy coding! 🚀🔮

👤 Follow us on [Twitter](https://twitter.com/AIMakerspace) and [LinkedIn](https://www.linkedin.com/company/ai-maker-space) for the latest news!
